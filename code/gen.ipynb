{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter, namedtuple\n",
    "\n",
    "token_tuple = namedtuple(\n",
    "        \"token\",\n",
    "        [\"index\", \"text\", \"lemma\", \"pos\", \"tag\", \"dep\", \"parent\", \"parent_index\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126799"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/passage_tokens.pkl\"\n",
    "with open(path, \"rb\") as f:\n",
    "    passage_tokens = pickle.load(f)\n",
    "len(passage_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_set = set(passage_tokens.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_token(t):\n",
    "    return t[3] == \"PUNCT\" or t[3] not in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"] or t[5] == \"ROOT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definitions(NOUN) -compound-> Ribbit(NOUN)\n",
      "Discuss(VERB) -dobj-> definitions(NOUN)\n",
      "with(ADP) -pobj-> community(NOUN)\n",
      "doing(VERB) -dobj-> best(ADJ)\n",
      "doing(VERB) -advcl-> make(VERB)\n",
      "make(VERB) -ccomp-> sure(ADJ)\n",
      "is(AUX) -nsubj-> content(NOUN)\n",
      "is(AUX) -acomp-> useful(ADJ)\n",
      "useful(ADJ) -conj-> accurate(ADJ)\n",
      "accurate(ADJ) -conj-> safe(ADJ)\n",
      "by(ADP) -pobj-> chance(NOUN)\n",
      "chance(NOUN) -acl-> spot(VERB)\n",
      "comment(NOUN) -amod-> inappropriate(ADJ)\n",
      "spot(VERB) -dobj-> comment(NOUN)\n",
      "use(VERB) -advcl-> navigating(VERB)\n",
      "through(ADP) -pobj-> website(NOUN)\n",
      "use(VERB) -dobj-> form(NOUN)\n",
      "use(VERB) -xcomp-> let(VERB)\n",
      "let(VERB) -ccomp-> know(VERB)\n",
      "use(VERB) -conj-> take(VERB)\n",
      "take(VERB) -dobj-> care(NOUN)\n",
      "take(VERB) -advmod-> shortly(ADV)\n"
     ]
    }
   ],
   "source": [
    "for pid, sent_dict in passage_tokens.items():\n",
    "    s_t = []\n",
    "    for sent_id, sent_tuples in sent_dict.items():\n",
    "        s_t += sent_tuples\n",
    "        # print(f\"sentence {sent_id+1} in passage {pid}:\")\n",
    "    for t_t in s_t:\n",
    "        if pass_token(t_t):\n",
    "            continue\n",
    "        current_token = token_tuple(*t_t)\n",
    "        parent_token = token_tuple(*s_t[current_token.parent_index])\n",
    "        print(f\"{parent_token.text}({parent_token.pos}) -{current_token.dep}-> {current_token.text}({current_token.pos})\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DocT5query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "t5_path = \"../model/docT5query/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(t5_path)\n",
    "dt5q = T5ForConditionalGeneration.from_pretrained(t5_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件collection.sampled.tsv 读取文件的格式为('pid', 'passage')\n",
      "正在处理文件train_sample_queries.tsv 读取文件的格式为('qid', 'query')\n",
      "正在处理文件train_sample_passv2_qrels.tsv 读取文件的格式为('qid', 'mark', 'pid', 'rating')\n",
      "正在处理文件val_2021_53_queries.tsv 读取文件的格式为('qid', 'query')\n",
      "正在处理文件val_2021_passage_top100.txt 读取文件的格式为('qid', 'mark', 'pid', 'rank', 'score', 'sys_id')\n",
      "正在处理文件val_2021.qrels.pass.final.txt 读取文件的格式为('qid', 'mark', 'pid', 'rating')\n",
      "正在处理文件test_2022_76_queries.tsv 读取文件的格式为('qid', 'query')\n",
      "正在处理文件test_2022_passage_top100.txt 读取文件的格式为('qid', 'mark', 'pid', 'rank', 'score', 'sys_id')\n",
      "正在处理文件test_2022.qrels.pass.withDupes.txt 读取文件的格式为('qid', 'mark', 'pid', 'rating')\n"
     ]
    }
   ],
   "source": [
    "from utils import Data\n",
    "data_processor = Data(root_path=\"../\")\n",
    "data_processor.read_in_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")what was the immediate impact of the success of the manhattan project?\n",
      "'Abstract. The pivotal engineering and scientific success of the Twentieth century was the Manhattan\\r\\n\\tProject. The Manhattan Project assimilated concepts and leaders from all scientific fields\\r\\n\\tand engineering disciplines to construct the first two atomic bombs.'\n",
      "1: What was the pivotal engineering and scientific success of the twenty-first century was the Manhattan Project?\n",
      "2: why did the nuclear atom bombs go off first?\n",
      "3: What is the role of engineering in the Twentieth century?\n",
      "4: Where can i find an article on the Manhattan Project of the late twentieth century..?\n",
      "5: The key economic and technological success of the twentieth century was the?\n"
     ]
    }
   ],
   "source": [
    "prefix = \"answer2question\"\n",
    "for pid, q_dict in data_processor.dataset['train_sample_passv2_qrels'].items():\n",
    "    qid = q_dict['qid']\n",
    "    print(data_processor.dataset['train_sample_queries'][qid]['query'])\n",
    "    print(data_processor.dataset['collection.sampled'][pid]['passage'])\n",
    "    text = prefix + \":\" + data_processor.dataset['collection.sampled'][pid]['passage']\n",
    "    input_ids = tokenizer.encode(text, max_length=384, truncation=True, return_tensors='pt')\n",
    "    outputs = dt5q.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=64,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=5)\n",
    "    for i in range(len(outputs)):\n",
    "        query = tokenizer.decode(outputs[i], skip_special_tokens=True)\n",
    "        print(f'{i + 1}: {query}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
