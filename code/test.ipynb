{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "path = \"../model/Reranker\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件collection.sampled.tsv 读取文件的格式为('pid', 'passage')\n",
      "正在处理文件train_sample_queries.tsv 读取文件的格式为('qid', 'query')\n",
      "正在处理文件train_sample_passv2_qrels.tsv 读取文件的格式为('qid', 'mark', 'pid', 'rating')\n",
      "正在处理文件val_2021_53_queries.tsv 读取文件的格式为('qid', 'query')\n",
      "正在处理文件val_2021_passage_top100.txt 读取文件的格式为('qid', 'mark', 'pid', 'rank', 'score', 'sys_id')\n",
      "正在处理文件val_2021.qrels.pass.final.txt 读取文件的格式为('qid', 'mark', 'pid', 'rating')\n",
      "正在处理文件test_2022_76_queries.tsv 读取文件的格式为('qid', 'query')\n",
      "正在处理文件test_2022_passage_top100.txt 读取文件的格式为('qid', 'mark', 'pid', 'rank', 'score', 'sys_id')\n",
      "正在处理文件test_2022.qrels.pass.withDupes.txt 读取文件的格式为('qid', 'mark', 'pid', 'rating')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['collection.sampled', 'train_sample_queries', 'train_sample_passv2_qrels', 'val_2021_53_queries', 'val_2021_passage_top100', 'val_2021.qrels.pass.final', 'test_2022_76_queries', 'test_2022_passage_top100', 'test_2022.qrels.pass.withDupes'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import Data\n",
    "data_processor = Data(root_path=\"../\")\n",
    "data_processor.read_in_memory()\n",
    "data_processor.dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what is personal essence'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_query_id = random.choice(tuple(data_processor.dataset[\"train_sample_queries\"].keys()))\n",
    "random_query = data_processor.dataset[\"train_sample_queries\"][random_query_id][\"query\"]\n",
    "random_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = {\n",
    "    v['rating']: data_processor.dataset[\"collection.sampled\"][pid]['passage']\n",
    "    for pid, v in data_processor.dataset[\"train_sample_passv2_qrels\"].items()\n",
    "    if v[\"qid\"] == random_query_id\n",
    "}\n",
    "len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is personal essence\n",
      "Answers: 'Concepts. When the personal essence is strong and in harmony with the individual there is health. Imbalance, lack of harmony or coherence in the personal essence is a precursor (even an actual cause) of subsequent disease.'\n"
     ]
    }
   ],
   "source": [
    "print(\"Query:\", random_query)\n",
    "print(\"Answers:\", answers['1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  2054,  2003,  3167, 11305,   102,  1005,  8474,  1012,  2043,\n",
       "          1996,  3167, 11305,  2003,  2844,  1998,  1999,  9396,  2007,  1996,\n",
       "          3265,  2045,  2003,  2740,  1012, 10047, 26657,  1010,  3768,  1997,\n",
       "          9396,  2030,  2522,  5886, 10127,  1999,  1996,  3167, 11305,  2003,\n",
       "          1037, 14988,  1006,  2130,  2019,  5025,  3426,  1007,  1997,  4745,\n",
       "          4295,  1012,  1005,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\n",
    "    random_query,\n",
    "    answers['1'],\n",
    "    return_tensors='pt',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6677]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "outputs.logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
